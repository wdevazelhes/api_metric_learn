{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API proposal for metric-learn to enhance scikit-learn compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook will propose a draft of API for distance metric learning algorithms that are compatible with scikit-learn, allowing to easily do pipelines, cross-validations, and connect with other scikit-learn objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # we still have warnings because of sparse arrays but will be fixed\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most metric algorithms that are not supervised in the classical way (with inputs X and labels y) take as input some pairs where each of these has a label (1 or 0, for positive pairs/negative pairs also called positive/negative constraint). One might want to evaluate the performance of a metric learning algorithm by doing cross-validation of a score (roc_auc score for instance), splitting between train/test on the **constraints** and not on the points. Therefore to be able to do so, we could create a ``Pairs`` object that would contain the information of the points (``X``) **and** the pairs (that can be represented as two lists: the list of the first indexes of pairs ``a``, and the second indexes ``b``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Pairs():\n",
    "\n",
    "    def __init__(self, X, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.X = X\n",
    "        self.shape = (len(a), X.shape[1])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # Note that to avoid useless memory consumption, when splitting we delete the points that are not used\n",
    "        a_sliced = self.a[item]\n",
    "        b_sliced = self.b[item]\n",
    "        unique_array = np.unique(np.concatenate([np.array(a_sliced), np.array(b_sliced)]))\n",
    "        inverted_index = self._build_inverted_index(unique_array)\n",
    "        pruned_X = self.X[unique_array].copy()  # copy so that the behaviour is always the same\n",
    "        rescaled_sliced_a = inverted_index[a_sliced].A.ravel()\n",
    "        rescaled_sliced_b = inverted_index[b_sliced].A.ravel()\n",
    "        return Pairs(pruned_X, rescaled_sliced_a, rescaled_sliced_b)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.shape\n",
    "\n",
    "    def __str__(self):\n",
    "        return np.stack([self.X[self.a], self.X[self.b]], axis=1).__str__()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return np.stack([self.X[self.a], self.X[self.b]], axis=1).__repr__()\n",
    "\n",
    "    def asarray(self):\n",
    "        return np.stack([self.X[self.a], self.X[self.b]], axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_inverted_index(unique_array):\n",
    "        inverted_index = csr_matrix((np.max(unique_array) + 1, 1), dtype=int)\n",
    "        inverted_index[unique_array] = np.arange(len(unique_array))[:, None]\n",
    "        return inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we would want a MetricLearner to be able to train on such a ``Pairs`` object and on the labels of constraints. We create a dummy metric learner just for testing purposes. It will just do one step in the direction of the gradient of pairwise distances of similar points. It can be used as a transfomer and as a classifier. \n",
    "- Its ``fit`` method takes as an input an object ``Pairs`` that represent pair of points, and an array-like (or list like) ``y`` that represent the labels of the constraints (positive constraint of negative constraint). (so ``len(y) == len(a) == lenb(b)``)\n",
    "- Then when ``decision_function`` is called with input a ``Pairs`` object, the metric learner will return the pairwise distances of the considered pairs. This will be useful to evaluate the cross-validation roc_auc score when splitting train/test on the pairs.\n",
    "- When ``transform`` is called with input a ``Pairs`` object, the metric learner will return a transformation of the **points** contained in the ``Pairs`` object. It can then return either a pairwise distance matrix on points, or an embedding of the points in the new space (if the algorithm can be expressed as such). This return type can be chosen at the creation of the Metric Learner by a flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyMetricLearner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, return_embedding=True):\n",
    "        self.A = None\n",
    "        self.return_embedding = return_embedding\n",
    "        \n",
    "    def fit(self, X_wrapped, y):\n",
    "        X, constraints = self.prepare_input(X_wrapped, y)\n",
    "        diffs = X[constraints[0]] - X[constraints[1]]\n",
    "        self.metric = diffs.T.dot(diffs)\n",
    "    \n",
    "    def fit_transform(self, X_wrapped, y):\n",
    "        self.fit(X_wrapped, y)\n",
    "        return self.transform(X_wrapped)\n",
    "\n",
    "    def decision_function(self, X_wrapped):\n",
    "        X_embedded = self.transform(X_wrapped)\n",
    "        squared_distances = np.sum((X_embedded[:, None] - X_embedded)**2,\n",
    "                                   axis=2)\n",
    "        return squared_distances[X_wrapped.a, X_wrapped.b]\n",
    "    \n",
    "    def transform(self, X_wrapped):\n",
    "        X_embedded = X_wrapped.X.dot(self.metric)\n",
    "        if self.return_embedding:\n",
    "            return X_embedded\n",
    "        else: \n",
    "            return np.sqrt(np.sum((X_embedded[:, None] - X_embedded)**2, axis=2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_input(X, y):\n",
    "        a = X.a[y==0]\n",
    "        b = X.b[y==0]\n",
    "        c = X.a[y==1]\n",
    "        d = X.b[y==1]\n",
    "        X = X.X\n",
    "        return X, [a, b, c, d]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a very simple synthetic dataset of pairs and labels for the pairs, and then the ``Pairs`` object, as well a Metric Learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #RNG = check_random_state(0)\n",
    "# X = np.random.randn(20, 5)\n",
    "X = np.arange(0, 20)[:, None] * np.ones((20, 4))\n",
    "a = np.array([1, 2, 4, 6, 7, 10, 11, 14, 17, 10])\n",
    "b = np.array([5, 3, 6, 7, 1, 16, 14, 16, 17, 11])\n",
    "y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n",
    "\n",
    "pairs = Pairs(X, a, b)\n",
    "dml = DummyMetricLearner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairs are a way to get an array of couples of points, without replicating a point in the computer's memory for every constraint this point is involved in. We can print it as we think of it: a list of couples of samples, in the form of a 3D numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  1.,   1.,   1.,   1.],\n",
       "        [  5.,   5.,   5.,   5.]],\n",
       "\n",
       "       [[  2.,   2.,   2.,   2.],\n",
       "        [  3.,   3.,   3.,   3.]],\n",
       "\n",
       "       [[  4.,   4.,   4.,   4.],\n",
       "        [  6.,   6.,   6.,   6.]],\n",
       "\n",
       "       [[  6.,   6.,   6.,   6.],\n",
       "        [  7.,   7.,   7.,   7.]],\n",
       "\n",
       "       [[  7.,   7.,   7.,   7.],\n",
       "        [  1.,   1.,   1.,   1.]],\n",
       "\n",
       "       [[ 10.,  10.,  10.,  10.],\n",
       "        [ 16.,  16.,  16.,  16.]],\n",
       "\n",
       "       [[ 11.,  11.,  11.,  11.],\n",
       "        [ 14.,  14.,  14.,  14.]],\n",
       "\n",
       "       [[ 14.,  14.,  14.,  14.],\n",
       "        [ 16.,  16.,  16.,  16.]],\n",
       "\n",
       "       [[ 17.,  17.,  17.,  17.],\n",
       "        [ 17.,  17.,  17.,  17.]],\n",
       "\n",
       "       [[ 10.,  10.,  10.,  10.],\n",
       "        [ 11.,  11.,  11.,  11.]]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do slicing on this object. It will indeed slice the pairs. This is useful for cross-validating metric learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 4.,  4.,  4.,  4.],\n",
       "        [ 6.,  6.,  6.,  6.]],\n",
       "\n",
       "       [[ 6.,  6.,  6.,  6.],\n",
       "        [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "       [[ 7.,  7.,  7.,  7.],\n",
       "        [ 1.,  1.,  1.,  1.]]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[2: 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do what we want: a cross-validation of the roc-auc score of the metric learner, splitting on the pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.  ,  0.75,  1.  ])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(dml, pairs, y, scoring='roc_auc', n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do clustering: passing the information of the labels of constraints in the y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, 1, 1, 1, 6, 6, 2, 2, 2, 7, 7, 0, 0, 0, 4, 4, 3, 3], dtype=int32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "pipe = make_pipeline(dml, KMeans())\n",
    "pipe.fit_predict(pairs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also chain a metric learner with unsupervised learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8320.,  2080.,  4420.,   520.,  6760.,  9620.,  3380.,  5460.],\n",
       "       [ 7800.,  1560.,  3900.,     0.,  6240.,  9100.,  2860.,  4940.],\n",
       "       [ 7280.,  1040.,  3380.,   520.,  5720.,  8580.,  2340.,  4420.],\n",
       "       [ 6760.,   520.,  2860.,  1040.,  5200.,  8060.,  1820.,  3900.],\n",
       "       [ 6240.,     0.,  2340.,  1560.,  4680.,  7540.,  1300.,  3380.],\n",
       "       [ 5720.,   520.,  1820.,  2080.,  4160.,  7020.,   780.,  2860.],\n",
       "       [ 5200.,  1040.,  1300.,  2600.,  3640.,  6500.,   260.,  2340.],\n",
       "       [ 4680.,  1560.,   780.,  3120.,  3120.,  5980.,   260.,  1820.],\n",
       "       [ 4160.,  2080.,   260.,  3640.,  2600.,  5460.,   780.,  1300.],\n",
       "       [ 3640.,  2600.,   260.,  4160.,  2080.,  4940.,  1300.,   780.],\n",
       "       [ 3120.,  3120.,   780.,  4680.,  1560.,  4420.,  1820.,   260.],\n",
       "       [ 2600.,  3640.,  1300.,  5200.,  1040.,  3900.,  2340.,   260.],\n",
       "       [ 2080.,  4160.,  1820.,  5720.,   520.,  3380.,  2860.,   780.],\n",
       "       [ 1560.,  4680.,  2340.,  6240.,     0.,  2860.,  3380.,  1300.],\n",
       "       [ 1040.,  5200.,  2860.,  6760.,   520.,  2340.,  3900.,  1820.],\n",
       "       [  520.,  5720.,  3380.,  7280.,  1040.,  1820.,  4420.,  2340.],\n",
       "       [    0.,  6240.,  3900.,  7800.,  1560.,  1300.,  4940.,  2860.],\n",
       "       [  520.,  6760.,  4420.,  8320.,  2080.,   780.,  5460.,  3380.],\n",
       "       [ 1040.,  7280.,  4940.,  8840.,  2600.,   260.,  5980.,  3900.],\n",
       "       [ 1560.,  7800.,  5460.,  9360.,  3120.,   260.,  6500.,  4420.]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pipe_unsupervised = make_pipeline(dml, PCA())\n",
    "pipe.fit_transform(pairs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Note that when creating a ``Pairs`` object, we keep the whole X inside it. This makes it possible to use the Metric Learner as a transformer on this X. However, when we do a splitting on the constraints, we will only keep the points from X that are present in the pairs of this slice, to be more memory efficient and to really create two datasets that are independent of one another.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
