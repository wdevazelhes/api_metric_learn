{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API proposal for metric-learn to enhance scikit-learn compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook will propose a draft of API for distance metric learning algorithms that are compatible with scikit-learn, allowing to easily do pipelines, cross-validations, and connect with other scikit-learn objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # we still have warnings because of sparse arrays but will be fixed\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many metric algorithms that are not supervised in the classical way (with inputs X and labels y) take as input some pairs where each of these has a label (1 or 0, for positive pairs/negative pairs also called positive/negative constraint). One might want to evaluate the performance of a metric learning algorithm by doing cross-validation of a score (roc_auc score for instance), splitting between train/test on the **constraints** and not on the points. Therefore to be able to do so, we could create a ``Pairs`` object that would contain the information of the points (``X``) **and** the pairs (that can be represented as two lists: the list of the first indexes of pairs ``a``, and the second indexes ``b``). This object would look like an array of couples of points, without replicating a point in the computer's memory for every constraint this point is involved in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Pairs():\n",
    "\n",
    "    def __init__(self, X, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.X = X\n",
    "        self.shape = (len(a), X.shape[1])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # Note that to avoid useless memory consumption, when splitting we delete the points that are not used\n",
    "        a_sliced = self.a[item]\n",
    "        b_sliced = self.b[item]\n",
    "        unique_array = np.unique(np.concatenate([np.array(a_sliced), np.array(b_sliced)]))\n",
    "        inverted_index = self._build_inverted_index(unique_array)\n",
    "        pruned_X = self.X[unique_array].copy()  # copy so that the behaviour is always the same\n",
    "        rescaled_sliced_a = inverted_index[a_sliced].A.ravel()\n",
    "        rescaled_sliced_b = inverted_index[b_sliced].A.ravel()\n",
    "        return Pairs(pruned_X, rescaled_sliced_a, rescaled_sliced_b)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.shape\n",
    "\n",
    "    def __str__(self):\n",
    "        return np.stack([self.X[self.a], self.X[self.b]], axis=1).__str__()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return np.stack([self.X[self.a], self.X[self.b]], axis=1).__repr__()\n",
    "\n",
    "    def asarray(self):\n",
    "        return np.stack([self.X[self.a], self.X[self.b]], axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_inverted_index(unique_array):\n",
    "        inverted_index = csr_matrix((np.max(unique_array) + 1, 1), dtype=int)\n",
    "        inverted_index[unique_array] = np.arange(len(unique_array))[:, None]\n",
    "        return inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we would want a MetricLearner to be able to train on such a ``Pairs`` object and on the labels of the corresponding ``Pairs``. We create a dummy metric learner just for testing purposes. It will just do one step in the direction of the gradient of pairwise distances of similar points w.r.t the metric matrix A. We will show how it can be used as a transformer as well as a classifier. \n",
    "- Its ``fit`` method takes as an input an object ``Pairs`` that represent pairs of points, and an array-like (or list like) ``y`` that represent the labels of the corresponding pairs (positive constraint of negative constraint). (so ``len(y) == len(a) == lenb(b) != len(X)``)\n",
    "- Then when ``decision_function`` is called with input a ``Pairs`` object, the metric learner will return the pairwise distances of the considered pairs. This will be useful to evaluate the cross-validation roc_auc score when splitting train/test on the pairs.\n",
    "- When ``transform`` is called with input a ``Pairs`` object, the metric learner will return a transformation of the **points** contained in the ``Pairs`` object. It can then return either a pairwise distance matrix on points, or an embedding of the points in the new space (if the algorithm can be expressed as such). This return type can be chosen at the creation of the Metric Learner by a flag.\n",
    "\n",
    "Therefore this algorithm could be a classifier of the **constraints** as well as a transformer of the **points**, as we will see later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DummyMetricLearner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, return_embedding=True):\n",
    "        self.A = None\n",
    "        self.return_embedding = return_embedding\n",
    "        \n",
    "    def fit(self, pairs, y):\n",
    "        X, constraints = self.prepare_input(pairs, y)\n",
    "        diffs = X[constraints[0]] - X[constraints[1]]\n",
    "        self.metric = diffs.T.dot(diffs)\n",
    "    \n",
    "    def fit_transform(self, pairs, y):\n",
    "        self.fit(pairs, y)\n",
    "        return self.transform(pairs)\n",
    "\n",
    "    def decision_function(self, pairs):\n",
    "        X_embedded = self.transform(pairs)\n",
    "        squared_distances = np.sum((X_embedded[:, None] - X_embedded)**2,\n",
    "                                   axis=2)\n",
    "        return squared_distances[pairs.a, pairs.b]\n",
    "    \n",
    "    def transform(self, pairs):\n",
    "        X_embedded = pairs.X.dot(self.metric)\n",
    "        if self.return_embedding:\n",
    "            return X_embedded\n",
    "        else: \n",
    "            return np.sqrt(np.sum((X_embedded[:, None] - X_embedded)**2, axis=2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_input(X, y):\n",
    "        a = X.a[y==0]\n",
    "        b = X.b[y==0]\n",
    "        c = X.a[y==1]\n",
    "        d = X.b[y==1]\n",
    "        X = X.X\n",
    "        return X, [a, b, c, d]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a very simple synthetic dataset of pairs and labels for the pairs, and then the ``Pairs`` object, as well a Metric Learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.arange(0, 20)[:, None] * np.ones((20, 4))  # points\n",
    "a = np.array([1, 2, 4, 6, 7, 10, 11, 14, 17, 10])  # first indices of pairs\n",
    "b = np.array([5, 3, 6, 7, 1, 16, 14, 16, 17, 11])  # second indices of pairs\n",
    "y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])  # labels of the constraints\n",
    "\n",
    "pairs = Pairs(X, a, b)\n",
    "dml = DummyMetricLearner(return_embedding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print pairs as we think of them: a list of couples of samples, in the form of a 3D numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  1.,   1.,   1.,   1.],\n",
       "        [  5.,   5.,   5.,   5.]],\n",
       "\n",
       "       [[  2.,   2.,   2.,   2.],\n",
       "        [  3.,   3.,   3.,   3.]],\n",
       "\n",
       "       [[  4.,   4.,   4.,   4.],\n",
       "        [  6.,   6.,   6.,   6.]],\n",
       "\n",
       "       [[  6.,   6.,   6.,   6.],\n",
       "        [  7.,   7.,   7.,   7.]],\n",
       "\n",
       "       [[  7.,   7.,   7.,   7.],\n",
       "        [  1.,   1.,   1.,   1.]],\n",
       "\n",
       "       [[ 10.,  10.,  10.,  10.],\n",
       "        [ 16.,  16.,  16.,  16.]],\n",
       "\n",
       "       [[ 11.,  11.,  11.,  11.],\n",
       "        [ 14.,  14.,  14.,  14.]],\n",
       "\n",
       "       [[ 14.,  14.,  14.,  14.],\n",
       "        [ 16.,  16.,  16.,  16.]],\n",
       "\n",
       "       [[ 17.,  17.,  17.,  17.],\n",
       "        [ 17.,  17.,  17.,  17.]],\n",
       "\n",
       "       [[ 10.,  10.,  10.,  10.],\n",
       "        [ 11.,  11.,  11.,  11.]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do slicing on this object. It will indeed slice the pairs. This is useful for cross-validating metric learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 4.,  4.,  4.,  4.],\n",
       "        [ 6.,  6.,  6.,  6.]],\n",
       "\n",
       "       [[ 6.,  6.,  6.,  6.],\n",
       "        [ 7.,  7.,  7.,  7.]],\n",
       "\n",
       "       [[ 7.,  7.,  7.,  7.],\n",
       "        [ 1.,  1.,  1.,  1.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[2: 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do what we want: a cross-validation of the roc-auc score of the metric learner, splitting on the pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.  ,  0.75,  1.  ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(dml, pairs, y, scoring='roc_auc', n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do clustering: for instance with a KMeans that is trained on embeddings returned by a metric learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 7, 7, 1, 1, 1, 5, 5, 4, 4, 4, 6, 6, 0, 0, 0, 2, 2, 2], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "pipe = make_pipeline(dml, KMeans())\n",
    "pipe.fit_predict(pairs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also chain a metric learner with unsupervised learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.94000000e+03,   8.46302823e-13],\n",
       "       [  4.42000000e+03,  -9.91412499e-14],\n",
       "       [  3.90000000e+03,  -7.45049640e-14],\n",
       "       [  3.38000000e+03,  -4.98686780e-14],\n",
       "       [  2.86000000e+03,  -2.52323921e-14],\n",
       "       [  2.34000000e+03,  -5.96106220e-16],\n",
       "       [  1.82000000e+03,  -8.62270007e-14],\n",
       "       [  1.30000000e+03,  -6.15907148e-14],\n",
       "       [  7.80000000e+02,  -3.69544289e-14],\n",
       "       [  2.60000000e+02,  -1.23181430e-14],\n",
       "       [ -2.60000000e+02,   1.23181430e-14],\n",
       "       [ -7.80000000e+02,   3.69544289e-14],\n",
       "       [ -1.30000000e+03,   6.15907148e-14],\n",
       "       [ -1.82000000e+03,   8.62270007e-14],\n",
       "       [ -2.34000000e+03,   5.96106220e-16],\n",
       "       [ -2.86000000e+03,   2.52323921e-14],\n",
       "       [ -3.38000000e+03,   4.98686780e-14],\n",
       "       [ -3.90000000e+03,   7.45049640e-14],\n",
       "       [ -4.42000000e+03,   9.91412499e-14],\n",
       "       [ -4.94000000e+03,   3.44311897e-13]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pipe_unsupervised = make_pipeline(dml, PCA(n_components=2))\n",
    "pipe_unsupervised.fit_transform(pairs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Note that when creating a ``Pairs`` object, we keep the whole X inside it. This makes it possible to use the Metric Learner as a transformer on this X. However, when we do a splitting on the constraints, we will only keep the points from X that are present in the pairs of this slice, to be more memory efficient and to really create two datasets that are independent of one another. This is OK because ``Pairs`` splitting will be used only in the context of cross-validation on pairs and not samples, for algorithms that only look at the pairs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This demo presents a simple possible API that allows to combine cross-validation scoring of the algorithm splitting on input pairs rather than points, while still allowing it to be used as a transformer in a pipeline. \n",
    "There are also other behaviours that are important in the API that are not discussed here:\n",
    "- Like the current metric-learn API, there could be a supervised version of each semi-supervised algorithm (ex: MMC **and**  MMC_Supervised), that would do the job of creating constraints and calling the semi-supervised algorithm under the hood.\n",
    "- Some algorithms should be able to be trained on constraints of type X[a] is more similar to X[b] than to X[c]\n",
    "- Here we consider we have either labels (and we use supervised algorithms), or label of constraints (and we use semi-supervised algorithms). We never have both, and cannot do pipelines where the constraints information would help resolve a supervised problem with external labels. One could manually combine constraints and labels, and do the training without using pipeline/cross-validation framework, but we may think of helper functions and/or API features that would make this easier\n",
    "- We could think of the hierarchy of objects, for instance supervised/[semi-supervised with pairs]/[other]..., [can return embedding]/[cannot] etc..., as well as the package organisation\n",
    "- We should have a look at Pipe Graph  https://mail.python.org/pipermail/scikit-learn/2018-January/002158.html "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
