{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # we still have warnings because of sparse arrays but will be fixed\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two ConstrainedDataset classes: one that makes a copy of X and one that keeps only a view of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConstrainedDatasetView():\n",
    "\n",
    "    def __init__(self, X, c):\n",
    "        self.c = c\n",
    "        self.X = X\n",
    "        self.shape = (len(c), X.shape[1])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # Note that to avoid useless memory consumption, when splitting we delete the points that are not used\n",
    "        c_sliced = self.c[item]\n",
    "        return ConstrainedDatasetView(self.X, c_sliced)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.shape\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.asarray().__str__()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.asarray().__repr__()\n",
    "\n",
    "    def asarray(self):\n",
    "        return self.X[self.c]\n",
    "    \n",
    "class ConstrainedDatasetCopy():\n",
    "\n",
    "    def __init__(self, X, c):\n",
    "        self.c = c\n",
    "        self.X = X.copy()\n",
    "        self.shape = (len(c), X.shape[1])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # Note that to avoid useless memory consumption, when splitting we delete the points that are not used\n",
    "        c_sliced = self.c[item]\n",
    "        X = self.X\n",
    "        return ConstrainedDatasetCopy(X, c_sliced)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.shape\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.asarray().__str__()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.asarray().__repr__()\n",
    "\n",
    "    def asarray(self):\n",
    "        return np.stack([self.X[self.c[:, 0].ravel()], self.X[self.c[:, 1].ravel()]], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's create a metric learner that could be problematic: a one where the operations on X will be done on the view. If we multithread like in a cross validation we may think that there will be some threads that will want to access the data at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ProblematicMetricLearner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, return_embedding=True):\n",
    "        self.A = None\n",
    "        self.return_embedding = return_embedding\n",
    "        \n",
    "    def fit(self, constrained_dataset, y):            \n",
    "        constraints = self.prepare_input(constrained_dataset, y)\n",
    "        diffs = constrained_dataset.X  # Here we use a view \n",
    "        self.metric = diffs.T.dot(diffs)\n",
    "        if constrained_dataset.X.__array_interface__['data'][0] == X.__array_interface__['data'][0]:\n",
    "            shared = 'is'\n",
    "        else: \n",
    "            shared = 'is not'\n",
    "        print('Memory {} shared with the initial object X.'.format(shared))\n",
    "    \n",
    "    def fit_transform(self, constrained_dataset, y):\n",
    "        self.fit(constrained_dataset, y)\n",
    "        return self.transform(constrained_dataset)\n",
    "    \n",
    "    def predict(self, constrained_dataset):\n",
    "        return self.decision_function(constrained_dataset)\n",
    "\n",
    "    def decision_function(self, constrained_dataset):\n",
    "        X_embedded = self.transform(constrained_dataset)\n",
    "        squared_distances = np.sum((X_embedded[:, None] - X_embedded)**2, axis=2)\n",
    "        return squared_distances[constrained_dataset.c[:, 0], constrained_dataset.c[:, 1]]\n",
    "    \n",
    "    def transform(self, constrained_dataset):\n",
    "        X_embedded = constrained_dataset.X.dot(self.metric)\n",
    "        if self.return_embedding:\n",
    "            return X_embedded\n",
    "        else: \n",
    "            return np.sqrt(np.sum((X_embedded[:, None] - X_embedded)**2, axis=2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_input(X, y):\n",
    "        a = X.c[y==0][:, 0]\n",
    "        b = X.c[y==0][:, 1]\n",
    "        c = X.c[y==1][:, 0]\n",
    "        d = X.c[y==1][:, 1]\n",
    "        X = X.X\n",
    "        return [a, b, c, d]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(1000, 10)\n",
    "c = np.random.randint(0, 1000, (20000, 2))\n",
    "y = np.random.randint(0, 2, 20000)\n",
    "view_dataset = ConstrainedDatasetView(X, c)\n",
    "copy_dataset = ConstrainedDatasetCopy(X, c)\n",
    "pml = ProblematicMetricLearner(return_embedding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that view_dataset.X is a view of X whereas copy_dataset is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(view_dataset.X.__array_interface__['data'][0] == X.__array_interface__['data'][0])\n",
    "print(copy_dataset.X.__array_interface__['data'][0] == X.__array_interface__['data'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, in a cross validation without multithreading, slices are made that will have a view of X in the case of ``view_dataset``, and algorithms only works on the view. For the ``copy_dataset``, copies are made at each slice and the algorithms work on views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val on the view object:\n",
      "Memory is shared with the initial object X.\n",
      "Memory is shared with the initial object X.\n",
      "Memory is shared with the initial object X.\n",
      "Cross val on the copied object:\n",
      "Memory is not shared with the initial object X.\n",
      "Memory is not shared with the initial object X.\n",
      "Memory is not shared with the initial object X.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.49628755,  0.50579987,  0.52288073])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Cross val on the view object:')\n",
    "cross_val_score(pml, view_dataset, y, scoring='roc_auc', n_jobs=1)\n",
    "print('Cross val on the copied object:')\n",
    "cross_val_score(pml, copy_dataset, y, scoring='roc_auc', n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will happen then if we multithread in the ``view_dataset`` case? (let's also check the case of ``copied_dataset`` but we know it is a copy from start so we already know algorithms will work on copies of the original X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val on the view object:\n",
      "Memory is not shared with the initial object X.\n",
      "Memory is not shared with the initial object X.\n",
      "Memory is not shared with the initial object X.\n",
      "Cross val on the copied object:\n",
      "Memory is not shared with the initial object X.\n",
      "Memory is not shared with the initial object X.\n",
      "Memory is not shared with the initial object X.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.49628755,  0.50579987,  0.52288073])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Cross val on the view object:')\n",
    "cross_val_score(pml, view_dataset, y, scoring='roc_auc', n_jobs=4)\n",
    "print('Cross val on the copied object:')\n",
    "cross_val_score(pml, copy_dataset, y, scoring='roc_auc', n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that for the ``view_dataset`` case, a copy appears to be created somewhere, which is cool because there is no bug to to concurrent data access. It is probably because of this: https://pythonhosted.org/joblib/parallel.html#working-with-numerical-data-in-shared-memory-memmaping\n",
    "```\n",
    "The arguments passed as input to the Parallel call are serialized and reallocated in the memory of each worker process.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
